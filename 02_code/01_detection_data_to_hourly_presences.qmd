---
title: "Chunk 01"
author: "lotte.pohl@vliz.be"
editor_options: 
  chunk_output_type: console
---

# Description

This code chunk fetches acoustic detection data of seabass from the European Tracking Network database and formats them into an dayly presence-(pseudo-)absence matrix.

## Input: explanation of input layers

* Acoustic telemetry data for the European seabass (*Dicentrarchus labrax*) in the Belgian Part of the North Sea (BPNS) from 2021-2022 will be downloaded from the [European Tracking Network (ETN) database](https://www.lifewatch.be/etn/) via the [etn R package](https://inbo.github.io/etn/articles/acoustic_telemetry.html).
 
* Specifically, detection data and metadata from deployments of acoustic receivers and from tagged animals will be fetched. The data will be filtered to only include the BPNS and the study period (2021-2022).


##  Workflow: explanation of workflow

* summarise data into dayly presences
* make pseudo-absences (don't resolve to individual level)

## Output: explanation of output layers
* 1 row per timestamp and 1 column per receiver station: na for inactive stations, 1 for presence, 0 for pseudoabsence

# Preparation

Load required scripts, set working directories.

```{r setup-folders}
# Source preparation script to create relative paths
# source("02_code/folder_structure.q")

#source necessary functions

knitr::opts_chunk$set(
  warning = FALSE,   # Suppress warnings
  message = FALSE    # Suppress messages
)
```

```{r load-packages}
# unsure still how .renv works with loading packages - 
#so for now loading them like this - change in the future

# install.packages(c("mregions2", "knitr", "dplyr", 
# "remotes", "leaflet", "sf", "ggplot2", "plotly"))
# remotes::install_github("inbo/etn@v2.3-beta", force = TRUE)

library(dplyr)
library(knitr)
library(readr)
library(tidyr)
library(etn)
library(purrr)
library(lubridate)
library(mapview)
library(leaflet)
library(mregions2)
library(sf)
library(giscoR)
library(ggplot2)
library(ggalt)
library(plotly)
options(scipen = 999)

library(here)
source(here("02_code/folder_structure.R"))

```

Next, we need to load our credentials for connecting to the ETN database.
We can do this by storing our username and password in a `.Renviron` file. This file should be located in the `02_code` folder of the project. The credentials are then read into the R environment using `readRenviron()`.
The file should contain the following lines:

`userid="your_etn_email_address"
pwd="your_etn_password"`

```{r connect-to-etndb}
# store userid and pwd in .Renviron file in the 02_code folder
readRenviron('.Renviron') 

Sys.getenv("userid")
Sys.getenv("pwd")
```

# Fetch data from etn database

Let's define some boundary conditions of the dataset we are working with:

```{r variables}

species <- "Dicentrarchus labrax"
start <- "2021-01-01"
end <- "2022-12-31"
BPNS <- sf::st_read(file.path("01_data","01_raw_data", "BPNS.gpkg"))  
# Belgian EEZ --> will be the bounding box
# BPNS <- 
#     mregions2::gaz_search(3293) |>
#     mregions2::gaz_geometry() |>
#     st_cast("POLYGON") |>
#     mutate(area = st_area(the_geom)) |>
#     # throw out the small river section of the BPNS
#     filter(area == max(area)) |>
#     select(-area)

# dataset used for this DUC
dataset <- "PhD_Goossens" 


# Europe geospatial data
Europe <-
  giscoR::gisco_get_countries(year = "2024", region = "Europe", resolution = 01) %>%
  dplyr::filter(!NAME_ENGL == "Russia")
```

## Tagged Animals

From the 'animals' dataframe, we need the column `release_date_time` to identify when the animal has been tagged.

```{r get_tagged_animals}

animals <- etn::get_animals(scientific_name = species,
                            animal_project_code = dataset) |>
              dplyr::mutate(tag_date = release_date_time |> as.Date(),
                            # remove double column entries
                            tag_serial_number = gsub(",.*","", tag_serial_number),
                            tag_type = gsub(",.*","", tag_type),
                            tag_subtype = gsub(",.*","", tag_subtype),
                            coustic_tag_id = gsub(",.*","", acoustic_tag_id))

```

## Tags used in the animals

In order to get the time period in which the tags were active, we need to get the `tags` dataframe from the ETN database.

```{r get_tags}

tags <- etn::get_tags(tag_serial_number = animals$tag_serial_number)

# make a simple dataframe with start and end date of each tag

tags_start_end <-
  animals |> 
    dplyr::select(tag_serial_number, tag_date) |>
    dplyr::left_join(tags |> 
    # only keep one row per tag_serial_number
  dplyr::distinct(tag_serial_number, battery_estimated_end_date) ) |>
    dplyr::mutate(battery_estimated_end_date = as.Date(battery_estimated_end_date)) |>
    dplyr::rename(tag_start = tag_date,
                  tag_end = battery_estimated_end_date) 
```

## Acoustic detections

```{r get-acoustic-detections}

detections_raw <- etn::get_acoustic_detections(start_date = start, 
                                           end_date = end,
                                           scientific_name = species,
                                           animal_project_code = dataset) |>
              dplyr::left_join(animals |> dplyr::select(animal_id, tag_date), by = "animal_id") 

detections_days <- detections_raw %>%
  dplyr::select(time = date_time,
                animal_id,
                deploy_latitude,
                deploy_longitude,
                station_name)%>%
  dplyr::mutate(time = lubridate::date(time))%>%
  group_by(time, station_name)%>%
  distinct(animal_id)%>%
  summarise(count = n())
            
detections_month <- 
  detections_raw |>
      dplyr::mutate(
        month = lubridate::month(date_time))  |>
      dplyr::group_by(month, station_name) |>
      dplyr::summarise(n_dets = n(), .groups = "drop",
      n_animals = n_distinct(animal_id) %>% as.integer(),
      lat = deploy_latitude |> mean(na.rm = T),
      lon = deploy_longitude |> mean(na.rm = T)) |>
  # make into sf object
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) 

# save the raw detections dataframe
base::saveRDS(detections_raw, "01_data/02_processed_data/detections.rds") #../../
base::saveRDS(detections_month, "01_data/02_processed_data/detections_month.rds")
```


```{r detections-map}
detections_map <- 
  ggplot() +
    # Europe
    geom_sf(data = Europe, fill = "lightgrey", color = "grey", alpha = 1) + 
    # Add BPNS polygons
    geom_sf(data = BPNS, fill = "blue", color = "blue", alpha = 0.1) + 
    # Add detections as red circle markers
    geom_sf(data = detections_month,
              aes(size = n_animals,color = n_dets),
              alpha = 1) +
    theme_bw() +
    labs(title = NULL, x = NULL, y = NULL) +
    coord_sf(xlim = c(2, 5), ylim = c(51, 52), expand = FALSE) +
    theme(axis.text = element_blank(),
          axis.ticks = element_blank()) +
    facet_wrap(~month, ncol = 3) 
detections_map

ggsave(detections_map, file = "04_results/01a_GAM/figures_tables/detections_map_month.png", width = 25, height = 25, units = 'cm')

```

## Deployments of acoustic receivers

We get out all deployments from the ETN database and only retain those within the boundaries of the Belgian Part of the North Sea (BPNS).
We also filter out the deployments that are not within our study period (i.e., the start and end date).

```{r get-acoustic_deployments}
deployments <-
  etn::get_acoustic_deployments() |>
  filter(
    # only retain deployments within study period
    as.Date(recover_date_time) > as.Date(start),
    as.Date(deploy_date_time) < as.Date(end),
    !is.na(deploy_longitude) & !is.na(deploy_latitude)
  ) |>
  mutate(lat = deploy_latitude, lon = deploy_longitude) |>
  # make into sf object
  sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) |>
  # filter out deployments outside of BPNS
  mutate(within_BPNS = sf::st_within(geometry, BPNS) |> lengths() > 0,
          deploy_date = as.Date(deploy_date_time)) |>
  filter(within_BPNS) |>
  select(-within_BPNS)

# save the raw deployments dataframe
base::saveRDS(deployments, "01_data/02_processed_data/deployments.rds") #../../
```

Quick exploration map

```{r deployments-map}

ggplot() +
  # Europe
  geom_sf(data = Europe, fill = "lightgrey", color = "grey", alpha = 1) + 
  # Add BPNS polygons
  geom_sf(data = BPNS, fill = "blue", color = "blue", alpha = 0.1) + 
  # Add deployments as red circle markers
  geom_sf(data = deployments,  # Replace with actual columns for coordinates
             color = "red", 
             size = 3, 
             alpha = 0.5) +
  # Customize the plot theme
  theme_minimal() +
  labs(title = "", x = "longitude", y = "latitude") +
  coord_sf(xlim = c(2, 5), ylim = c(51, 52), expand = FALSE)

```

# Workflow

## Step 1
Create a time index: a dataframe with a row for each day between the start and end date.

```{r create-time-index}
# Create a time index for each day from start to end
time_index <- 
  tibble(time = seq(
    from = as.Date(start),
    to = as.Date(end),
    by = "day"))

# time_index <- 
#   tibble(time = seq(
#     from = paste(start, "00:00:00") |> 
#       as.POSIXct(format="%Y-%m-%d %H:%M:%S", tz = "UTC"), 
#     to = paste(end, "00:00:00") |> 
#       as.POSIXct(format="%Y-%m-%d %H:%M:%S", tz = "UTC"), 
#     by = "day")) 
```

### make active tags per timestep dataframe

We want to use the number of active tags per timestep as an offset in the model later on, since this is a bias introduced into the dataset by the nature of acoustic telemetry data.
```{r get_active_tags_per_timestep}

active_tags <- time_index |>
  mutate(
    n_active_tags = rowSums(
      outer(time, tags_start_end$tag_start, `>=`) &
      outer(time, tags_start_end$tag_end, `<=`)
    )
  )
base::saveRDS(active_tags, "01_data/02_processed_data/active_tags.rds")
# # plot showing the active tags
# ggplot() +
#   geom_vline(data = active_tags, aes(xintercept = time, color = n_active_tags), alpha = 0.5) +
#   ggalt::geom_dumbbell(data = tags_start_end, aes(y = tag_serial_number, x = tag_start, xend = tag_end), size=2) +
#   labs(x = "active tag period") +
#   theme_bw()

# ggsave(file.path(fig_gam_dir, "active_tags.png"), width = 15, height = 25, units = 'cm')

```


## Step 2: 

Make df with 1 row for each day and station combination.

```{r stations-days}
stations <- 
  deployments |>
    dplyr::group_by(station_name) |>
    dplyr::summarise(deploy_latitude = deploy_latitude |> mean(na.rm = T),
                     deploy_longitude = deploy_longitude |> mean(na.rm = T))

stations_days <- 
  time_index |>
    crossing(station_name = stations$station_name)

stations_days |> head() |> knitr::kable()
```

## Step 3


## Step 4

Make df with 1 row for each day with an active deployment

```{r deployments-days}
deployments_days <-
  time_index |>
    left_join(deployments |>
                dplyr::select(station_name, deploy_date_time, recover_date_time) |>
              sf::st_drop_geometry() |>
              dplyr::mutate(value_deploy = 0,
                            deploy_date = as.Date(deploy_date_time),
                            recover_date = as.Date(recover_date_time)),
    by = join_by(between(time, deploy_date, recover_date)))|>
    dplyr::select(-c(deploy_date_time, recover_date_time, deploy_date, recover_date))

deployments_days |> slice_sample(n = 10) |> knitr::kable()

base::saveRDS(deployments_days, "01_data/02_processed_data/deployments_days.rds")
```


## Step 5

Merge all dataframes into one dataframe with 1 row for each day and each station, and fill the value column with presences (1) or absences (0). Then pivot wider to get a wide format with 1 column per station.

```{r merge-dataframes}
detections_day <- left_join(deployments_days, detections_days, by = c("time", "station_name"))%>%
  mutate(across(count, ~ replace_na(.,0)))%>%
  dplyr::select(-value_deploy)
detections_day <- left_join(detections_day, active_tags, by = "time")
saveRDS(detections_day, file = "01_data/02_processed_data/detections_day.rds")

#If we use counts per day instead
output_chunk01 <- 
  detections_day |>
  dplyr::left_join(stations, by = join_by(station_name))
  dplyr::left_join(active_tags, by = join_by(time))

output_chunk01 |> slice_sample(n = 10) |> knitr::kable()
```

```{r troubleshoot-multipoint}
# I realised that the geometry column is a multipoint, so I need to convert it to a point --> do later, not of relevance

# output_chunk01_subset <- chunk01 |> slice_sample(n = 20)

# output_chunk01_subset |> knitr::kable()

# output_chunk01_subset_clean <- output_chunk01_subset |>
#   mutate(
#     geometry = map(geometry, ~ {
#       if (st_geometry_type(.x) == "MULTIPOINT") {
#         st_point(.x[1, ])  # extract first row as numeric vector
#       } else {
#         .x
#       }
#     })
#   ) |>
#   st_as_sf()

# output_chunk01_subset_clean |> knitr::kable()
```

# Save Outputs

Store processed data and results for further use.

```{r save-outputs}
# Save the output as a CSV file
readr::write_csv(output_chunk01, file.path(processed_dir, "output_chunk01.csv"))  # only keeps NA vals

# as RDS
base::saveRDS(output_chunk01,file.path(processed_dir, "output_chunk01.rds")) # keeps NA, 0 and 1 vals
# base::saveRDS(stations, "../../01_data/02_processed_data/stations.rds")

# save the raw detections and deployments dataframes
base::saveRDS(detections_raw, file.path(processed_dir, "detections.rds"))
base::saveRDS(deployments, file.path(processed_dir, "deployments.rds"))
base::saveRDS(stations, file.path(processed_dir, "stations.rds"))

```

# Documentation

Extract .R components from all .qmd files in the folder

```{r extract-R-from-qmd}

# source("02_code/01_boosted_regression_trees/00_extract_R_from_qmd.R")
source("00_extract_R_from_qmd.R") #when rendering pdf doc
```
